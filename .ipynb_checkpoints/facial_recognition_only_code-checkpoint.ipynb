{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imutils import face_utils\n",
    "import numpy as np\n",
    "import imutils\n",
    "import argparse\n",
    "import dlib\n",
    "import cv2\n",
    "import random\n",
    "from PIL import Image\n",
    "# import the necessary packages\n",
    "from imutils.face_utils import FaceAligner\n",
    "from imutils.face_utils import rect_to_bb\n",
    "import argparse\n",
    "import imutils\n",
    "import dlib\n",
    "import cv2\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# construct the argument parser and parse the arguments\n",
    "#ap = argparse.ArgumentParser()\n",
    "#ap.add_argument(\"-p\", \"--shape-predictor\", required=True,help=\"path to facial landmark predictor\")\n",
    "#%tb ap.add_argument(\"-i\", \"--image\", required=True,help=\"path to input image\")\n",
    "#%tb args = vars(ap.parse_args())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "fa = FaceAligner(predictor, desiredFaceWidth=256)\n",
    "\n",
    "# Face Align Function\n",
    "def faceAlign(image):\n",
    "    #image = cv2.imread(img)\n",
    "    image = imutils.resize(image, width=800)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    rects = detector(gray, 2)\n",
    "    faceAligned = []\n",
    "    for rect in rects:\n",
    "        faceAligned = fa.align(image, gray, rect)\n",
    "        #print(rect)\n",
    "        #plt.imshow(faceAligned)\n",
    "    \n",
    "    return faceAligned\n",
    "\n",
    "\n",
    "image = cv2.imread('dataset_cropped/Abhishek Bachan/AbhishekBachan71.jpg')\n",
    "#plt.imshow(image)\n",
    "#plt.imshow(faceAlign(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os                                                                                                             \n",
    "\n",
    "def list_files(dir):\n",
    "    r = []\n",
    "    for root, dirs, files in os.walk(dir):\n",
    "        for name in files:\n",
    "            r.append(os.path.join(root, name))\n",
    "    return r  \n",
    "\n",
    "image_paths=list_files('dataset_cropped')\n",
    "#print(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finding unique labesl and encoding them\n",
    "\n",
    "all_labels = [imag.split('/')[-2] for imag in image_paths]\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(all_labels)\n",
    "labels_enc = le.transform(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abhishek Bachan', 'Alex Rodriguez', 'Ali Landry', 'Alyssa Milano', 'Anderson Cooper', 'Anna Paquin', 'Audrey Tautou', 'Barack Obama', 'Ben Stiller', 'Christina Ricci', 'Clive Owen', 'Cristiano Ronaldo', 'Daniel Craig', 'Danny Devito', 'David Duchovny', 'Denise Richards', 'Diane Sawyer']\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]\n"
     ]
    }
   ],
   "source": [
    "unique_labels = list(le.classes_)\n",
    "print(unique_labels)\n",
    "print(le.transform(unique_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def read_images(image_paths):\n",
    "    images = [];\n",
    "    labels = [];\n",
    "    for img_path in image_paths:\n",
    "        image = faceAlign(cv2.imread(img_path));\n",
    "        if image is not None:\n",
    "            images.append(image)\n",
    "            labels.append(img_path.split('/')[1])\n",
    "    return images,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Encoding labels\n",
    "\n",
    "\n",
    "#list(le.classes_)\n",
    "\n",
    "def data_gen(batch_size=30):\n",
    "    while True:\n",
    "        image_paths=list_files('dataset_cropped')\n",
    "        sample_dt=random.sample(image_paths,int(batch_size))\n",
    "        \n",
    "        images,labels=read_images(sample_dt)\n",
    "        #images,measurements=random_append_augment_images(images,measurements)\n",
    "        #print('ya')\n",
    "        images=np.asarray(images)\n",
    "        labels=np.asarray(labels)\n",
    "        #print('images shape',images.shape)\n",
    "        #print('measurements shape',measurements.shape)\n",
    "        ind = np.random.choice(images.shape[0], int(batch_size), replace=False)\n",
    "        images=images[ind,:,:,:]\n",
    "        labels=le.transform(labels[ind])\n",
    "        X=images\n",
    "        y=labels\n",
    "        \n",
    "        yield X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_length = 15\n",
    "image_paths=list_files('dataset_cropped')\n",
    "sample_dt=random.sample(image_paths,int(sample_length))\n",
    "images,labels=read_images(sample_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ya\n"
     ]
    }
   ],
   "source": [
    "c,v = next(data_gen(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alexnet\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten,Dense, Lambda\n",
    "# There was a problem with original Keras progress bar due to which Notebook used to hang.\n",
    "# Changing the progress bar with another version of it so as to get the code working in Notebook. Look at model.fit command on how its used\n",
    "#from keras_tqdm import TQDMNotebookCallback \n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Activation, Cropping2D, ELU\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "input_shape=(image.shape[0],image.shape[1],3)\n",
    "model=Sequential()\n",
    "model.add(Lambda(lambda x:x/255.0-0.50,input_shape=input_shape))\n",
    "model.add(Conv2D(3,1,1,subsample=(1,1),border_mode=\"valid\",init='he_normal'))\n",
    "model.add(ELU())\n",
    "model.add(Conv2D(24,5,5,subsample=(2,2),border_mode=\"valid\",init='he_normal'))\n",
    "model.add(ELU())\n",
    "model.add(Conv2D(36,5,5,subsample=(2,2),border_mode=\"valid\",init='he_normal'))\n",
    "model.add(ELU())\n",
    "model.add(Dropout(.4))\n",
    "model.add(Conv2D(48,5,5,subsample=(2,2),border_mode=\"valid\",init='he_normal'))\n",
    "model.add(ELU())\n",
    "model.add(Conv2D(64,3,3,subsample=(1,1),border_mode=\"valid\",init='he_normal'))\n",
    "model.add(ELU())\n",
    "model.add(Conv2D(64,3,3,subsample=(1,1),border_mode=\"valid\",init='he_normal'))\n",
    "model.add(Dropout(.3))\n",
    "model.add(ELU())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1164,init='he_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(100,init='he_normal'))\n",
    "model.add(ELU())\n",
    "model.add(Dense(50,init='he_normal'))\n",
    "model.add(ELU())\n",
    "model.add(Dense(10,init='he_normal'))\n",
    "model.add(ELU())\n",
    "model.add(Dense(1,init='he_normal'))\n",
    "\n",
    "model.compile(loss='mse',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "batch_size=30\n",
    "epochs=4\n",
    "gen_train=data_gen(15)\n",
    "gen_valid=data_gen(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-8ad7eb723ba1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_val_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    933\u001b[0m                                         \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m     def evaluate_generator(self, generator, val_samples,\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1520\u001b[0m                             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1522\u001b[0;31m                             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(gen_train, samples_per_epoch=25,nb_epoch=epochs,validation_data=gen_valid,nb_val_samples=25, max_q_size=25, nb_worker=4, pickle_safe=True, verbose =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
